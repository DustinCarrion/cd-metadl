{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deNiSKRkFh9W"
      },
      "source": [
        "<hr style=\"border-width:2px;border-color:#84C7F7\">\n",
        "<center><h1> Cross-Domain MetaDL Competition </h1></center>\n",
        "<center><h2>  Any-way Any-shot Learning </h2></center>\n",
        "<hr style=\"border-width:2px;border-color:#84C7F7\">\n",
        "\n",
        "<center>\n",
        "<button type=\"button\" style=\"background-color: #4CAF50; border: none; color: white; padding: 15px 32px; text-align: center; text-decoration: none; display: inline-block; font-size: 16px; border-radius: 12px;\"><a href=\"https://codalab.lisn.upsaclay.fr/competitions/3627\" style=\"text-decoration: none; color: inherit;\">Competition Site</a></button>\n",
        "<button type=\"button\" style=\"background-color: #4CAF50; border: none; color: white; padding: 15px 32px; text-align: center; text-decoration: none; display: inline-block; font-size: 16px; border-radius: 12px;\"><a href=\"mailto:metalearningchallenge@googlegroups.com\" style=\"text-decoration: none; color: inherit;\">Contact us</a></button>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFLvWWU2Fh9k"
      },
      "source": [
        "This tutorial is divided by level of complexity. However, all the levels are sequential. Therefore, each level assumes that you have already completed the previous levels.\n",
        "\n",
        "* [1) **Beginner level**](#beginner) (no prerequisite): Approximate time required **10 minutes**.\n",
        "* [2) **Intermediate level**](#intermediate) (some knowledge of Python and meta-learning): Approximate time required **20 minutes**.\n",
        "* [3) **Advanced level**](#advanced) (solid knowledge of Python and meta-learning) : Approximate time required **30 minutes**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TObbnPrFh9l"
      },
      "source": [
        "<a name='beginner'></a>\n",
        "# 1 Beginner Level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhSVy0y2Fh9m"
      },
      "source": [
        "<a name='beginner_intro'></a>\n",
        "## 1.1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtOHFwj8bS7b"
      },
      "source": [
        "The goal of this competition is to produce a **Learner** that can quickly adapt to new tasks from multiple domains using only a few examples. Thus, you will have to tackle the **any-way any-shot learning problem**. The few-shot learning problems are often referred as *N*-way *k*-shots problems. This name refers to the configuration of the tasks at **meta-test time**. Each task consists of a small training set and a small test set, referred to as **support** and **query** sets, respectively. The number of **ways** *N* denotes the number of classes in a task that represents an image classification problem. The number of **shots** *k* denotes the number of examples per class in the **support set**. In our case, we focus on the **any-way any-shot** setting. In other words, the tasks at meta-test time represent image classification problems with a number of classes varying from 2 to 20, and the **support set** contains 1 to 20 labeled examples per class, *i.e.*, $N \\in [2, 20]$, and $k \\in [1, 20]$. Moreover, as this is a Cross-Domain competition, the tasks are drawn from one of 10 datasets from different domains. Thus, at meta-test time, your submission may be tested in the following way:\n",
        "- **Test task 1:** 5-ways 1-shot task from Dataset 9.\n",
        "- **Test task 2:** 3-ways 15-shots task from Dataset 3.\n",
        "- **Test task 3:** 12-ways 4-shots task from Dataset 9.\n",
        "- **Test task 4:** 2-ways 8-shots task from Dataset 8.\n",
        "- $\\vdots$\n",
        "\n",
        "Cross-Domain MetaDL is an online competition with code submission, *i.e.*, you need to provide your submission as raw Python code that will be evaluated on the competition platform by 2 programs called \"ingestion\" and scoring\" program as illustrated in the following figure.\n",
        "\n",
        "<a name='beginner_workflow'></a>\n",
        "<center>\n",
        "<img src=\"./imgs/evaluation_flow.png\" alt=\"Evaluation workflow of the competition\" width=1500>\n",
        "</center>\n",
        "\n",
        "The details of `Your Submission` are explained in the [next section](#beginner_submission). Now, let's briefly describe the stages of the `Ingestion Program`:\n",
        "\n",
        "* **Meta-training**: It is used to meta-train your algorithm with a large meta-training set including fully labeled data from 10 datasets that are different from those used for meta-testing, but from similar domains. The outcome of meta-training is a learner ready to tackle new tasks from the same domain, but different datasets. This is the stage you control the most. You can choose to use data in the form of **tasks** or **batches** (see [Example Figure](#beginner_data_format)). In both cases, you can select your preferred configurations.\n",
        "* **Meta-validation (optional)**: It can be used to evaluate your learners (and select the best one) on new test tasks in the process of learning (these test tasks are only for validation purposes, not for ranking participants). At this stage, the data is always in the form of **tasks**, but you can still select your preferred configurations.\n",
        "* **Meta-testing**: We will use it to evaluate your algorithm's ability to quickly adapt to new unseen any-way any-shot tasks from several domains. At this stage, you have no control, the data is always in the form of **any-way any-shot tasks** with $N \\in [2, 20]$ and $k \\in [1, 20]$. It is worth mentioning that at this stage, the test labels are hidden to your selected learner.\n",
        "\n",
        "Example of **task** and **batch** data format:\n",
        "\n",
        "<a name='beginner_data_format'></a>\n",
        "<center>\n",
        "<img src=\"./imgs/train_settings.png\" alt=\"Example of data format\" width=650>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YOnhc1jFh9n"
      },
      "source": [
        "<a name='beginner_submission'></a>\n",
        "## 1.2 Submission Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBUONBbHb3Nj"
      },
      "source": [
        "A **submission** is a zip file containing at least three files: `api.py`, `model.py`, and `metadata`. The former contains the API specification we defined for the competition, you just have to copy this file into your submission folder (you can find this file in any given baseline's folder). For this part of the tutorial, you do not need to understand the API, we will explain the details in the [intermediate tutorial](#intermediate). `model.py` is your implementation of the API, *i.e.*, it contains your proposed approach for the any-way any-shot problem. `metadata` is just an empty file for the competition server to work properly, you simply add it to your folder without worrying about it (you can find this file in any given baseline's folder). Lastly, you are free to add any additional files you may need for your algorithm.\n",
        "\n",
        "Now, let's look at a real example of the `model.py`. Please see how a random seed is set at the beginning of the file to guarantee reproducibility. In this competition, it is MANDATORY that you seeded your algorithms. Make sure there are NO OTHER HARD CODED RANDOM SEEDS in your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuxQ8uUMFh9q"
      },
      "outputs": [],
      "source": [
        "from main_utils import display\n",
        "\n",
        "display(\"baselines/random/model.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyuo9dAzFh9u"
      },
      "source": [
        "Now you are ready to zip your code and submit it on the [CodaLab platform](https://codalab.lisn.upsaclay.fr/competitions/3627#participate) from July 1st. As an example, we zip the folder with the random baseline shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCM_I1FuFh9v"
      },
      "outputs": [],
      "source": [
        "from main_utils import zipdir\n",
        "\n",
        "model_dir = \"baselines/random/\"\n",
        "submission_filename = \"example_submission.zip\"\n",
        "zipdir(submission_filename, model_dir)\n",
        "print(f\"Your file {submission_filename} is ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFBU-YrfFh9w"
      },
      "source": [
        "## 1.3 Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0W5H_kncdFm"
      },
      "source": [
        "Congratulations you finished the beginner tutorial. At this point, you should have a valid submission for this competition (random baseline). Now, you can continue with the [Intermediate tutorial](#intermediate).\n",
        "\n",
        "If you run into bugs or issues when using this starting kit, please create issues on the [*Issues* page](https://github.com/DustinCarrion/cd-metadl/issues) of this competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu9gzf08Fh9x"
      },
      "source": [
        "<hr style=\"border-width:4px;border-color:#84C7F7\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJJJi7n8Fh9x"
      },
      "source": [
        "<a name='intermediate'></a>\n",
        "# 2 Intermediate Level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFWj3JMAxUt3"
      },
      "source": [
        "We assume you read the [beginner tutorial](#beginner). We now go over more advanced topics: \n",
        "\n",
        "1. [Public Data](#intermediate_public_data)\n",
        "2. [Configuring the environment](#intermediate_env)\n",
        "3. [Challenge API](#intermediate_API)\n",
        "4. [Data Configuration File](#intermediate_config)\n",
        "5. [Fine-tuning Baseline](#intermediate_finetuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caHNrq4nd_Zy"
      },
      "source": [
        "<a name='intermediate_public_data'></a>\n",
        "## 2.1 Public Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQSEJxqneBE1"
      },
      "source": [
        "This competition uses 30 datasets from the newly created [Meta Album meta-dataset](https://meta-album.github.io). These datasets belong to 10 domains:\n",
        "\n",
        "* Large Animals\n",
        "* Small Animals\n",
        "* Plants\n",
        "* Plant Diseases\n",
        "* Microscopy\n",
        "* Remote Sensing\n",
        "* Vehicles\n",
        "* Manufacturing\n",
        "* Human Actions\n",
        "* Optical Character Recognition\n",
        "\n",
        "Each phase of the competition (Public, Feedback, Final) has its own group of datasets. Each group of datasets is composed of 1 dataset per domain, *i.e.*, 10 datasets in total. Additionally, each dataset is uniformly formatted following [these instructions](https://github.com/ihsaan-ullah/meta-album/tree/master/DataFormat). The [Public Data for this competition](https://codalab.lisn.upsaclay.fr/competitions/3627#participate-get_starting_kit) corresponds to the first group of datasets. \n",
        "\n",
        "We will use the Public Data in this tutorial to better understand the difference between **tasks** and **batches** introduced in the [beginner tutorial](#beginner_data_format). For this reason, before continuing with the next sections, please make sure you have the Public Data for which you have two options:\n",
        "\n",
        "1. Download the Public Data from the `Files` tab of the [Competition Site](https://codalab.lisn.upsaclay.fr/competitions/3627#participate-get_starting_kit), copy the downloaded zip in this folder and unzip it.\n",
        "\n",
        "2. Execute the following cell. It will install the `requests` library (if you do not have it already)  to download the Public Data. Take into account that this step will get all 10 public datasets (more than 66,000 files)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMXxD-GZgPBV"
      },
      "outputs": [],
      "source": [
        "from main_utils import download_public_data\n",
        "\n",
        "download_public_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify that the Public Data is correct, please execute the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from main_utils import verify_public_data\n",
        "\n",
        "verify_public_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHIoPH31ercm"
      },
      "source": [
        "<a name='intermediate_API'></a>\n",
        "## 2.2 Challenge API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0oG7J2Dyd2m"
      },
      "source": [
        "As explained in the [beginner tutorial](#beginner_submission), your solutions must follow our defined API. The following figure shows the details of the API.\n",
        "\n",
        "<center>\n",
        "<img src=\"./imgs/API.png\" alt=\"Challenge API\" width=600>\n",
        "</center>\n",
        "\n",
        "Let's describe each class:\n",
        "* **MetaLearner**: It contains the meta-algorithm logic. The `meta_fit(meta_train_generator, meta_valid_generator)` method has to be overwritten with your own meta-learning algorithm. In general, a **MetaLearner** is meta-trained and returns a **Learner**, to be meta-tested. **IMPORTANT**: It is not mandatory to meta-learn in the `meta_fit(meta_train_generator, meta_valid_generator)` method, you can return a \"hard-coded\" learning algorithm (**Learner**).\n",
        "* **Learner**: It encapsulates the logic to learn from a new unseen task. Several methods need to be overwritten: \n",
        "  * `fit(support_set)`: Fits the **Learner** to a new unseen task.\n",
        "  * `save(path)`: Saves your **Learner** in the specified path. \n",
        "  * `load(path)`: Loads your **Learner** from the file(s) you created in `save(path)`.\n",
        "  \n",
        "  In general, a **Learner** is trained on the support set of a meta-test task and returns a **Predictor**, to be tested on the unlabelled query set of that same task.\n",
        "\n",
        "* **Predictor**: It contains the logic of your **Learner** to make predictions once it is fitted. The `predict(query_set)` method receives the unallabeled query set of a task and processes it with the trained **Learner** produced by the `fit(support_set)` method. The **Predictor** can return the raw logits (matrix), the probabilities for each label (matrix), or the predicted labels (1D array).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='intermediate_env'></a>\n",
        "## 2.3 Configuring the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The remaining sections of this tutorial require the dependencies specified in the `requirements.txt` to be installed. You can:\n",
        "\n",
        "1. Execute the following cell to install all the dependencies in your current session/environment.\n",
        "2. Follow [these instructions](https://github.com/DustinCarrion/cd-metadl/blob/main/optional_instructions/anaconda_config.md) to configure a conda environment.\n",
        "3. Follow [these instructions](https://github.com/DustinCarrion/cd-metadl/blob/main/optional_instructions/docker_config.md) to use docker.\n",
        "\n",
        "**Note:** From the above list, you can follow any of the options, you do not have to follow all of them. Moreover, you just have to follow this process once, next time you access the tutorial, you can skip this section. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you want to install the dependencies in your current session/environment \n",
        "# please run this cell, otherwise, you can continue with the tutorial\n",
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9b5g2t0Fh9z"
      },
      "source": [
        "<a name='intermediate_config'></a>\n",
        "## 2.4 Data Configuration File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF2dWVANdECs"
      },
      "source": [
        "As shown in the [previous section](#intermediate_API), the `meta_fit` method of the **MetaLearner** receives two data generators (`meta_train_generator` and `meta_valid_generator`). In the [beginner tutorial](#beginner_intro) we mentioned that you can select your preferred configurations for them. Thus, an additional file that you may want to include in your submission is `config.json`. This file can be used to specify all the details of the data format that you would like to use. Here is an example of a config file with all the possible configurations that you can define:\n",
        "\n",
        "**Content of a `config.json` file**:\n",
        "```bash\n",
        "{\n",
        "    \"train_data_format\": \"task\",\n",
        "    \"batch_size\": null,\n",
        "    \"train_config\": {\n",
        "        \"N\": 10,\n",
        "        \"min_N\": null,\n",
        "        \"max_N\": null,\n",
        "        \"k\": null,\n",
        "        \"min_k\": 5,\n",
        "        \"max_k\": 10,\n",
        "        \"query_images_per_class\": 5\n",
        "    },\n",
        "    \"validation_datasets\": 2,\n",
        "    \"valid_config\": {\n",
        "        \"N\": null,\n",
        "        \"min_N\": 2,\n",
        "        \"max_N\": 5,\n",
        "        \"k\": 20,\n",
        "        \"min_k\": null,\n",
        "        \"max_k\": null,\n",
        "        \"query_images_per_class\": 10\n",
        "    }\n",
        "}\n",
        "```\n",
        "The above example configuration corresponds to:\n",
        "\n",
        "* `meta_train_generator`: 10-ways any-shot tasks with with $k \\in [5, 10]$ and 5 images per class for the query set.\n",
        "\n",
        "* `meta_valid_generator`: any-way 20-shots tasks with with $N \\in [2, 5]$ and 10 images per class for the query set.\n",
        "\n",
        "Additionally, 2 of the available datasets will be used for the meta-validation split and the remaining datasets will be used for the meta-training split. \n",
        "\n",
        "For clarity, the available configurations are:\n",
        "\n",
        "- `train_data_format`: Format for the training data, it can be \"task\" or \"batch\".\n",
        "- `batch_size`: Batch size for the generated batches. Only used if `train_data_format` is \"batch\", in which case `train_config` is ignored. It cannot be less than 1.\n",
        "- `train_config`: Configuration for the training data. Only used if `train_data_format` is \"task\", in which case `batch_size` is ignored.\n",
        "  - `N`: Fixed number of ways for the generated tasks at meta-train time. It cannot be less than 2.\n",
        "  - `min_N`: Lower bound for the number of ways for the generated tasks at meta-train time. Only used if `N` is `null`. It cannot be less than 2 and must be less than or equal to `max_N`.\n",
        "  - `max_N`: Upper bound for the number of ways for the generated tasks at meta-train time. Only used if `N` is `null`. It must be greater or equal to `min_N`.\n",
        "  - `k`: Fixed number of shots for the generated tasks at meta-train time. If you would like to use any-shot configuration, then you have to define this parameter as `null`. It cannot be less than 1.\n",
        "  - `min_k`: Lower bound for the number of shots for the generated tasks at meta-train time. Only used if `k` is `null`. It cannot be less than 1 and must be less than or equal to `max_k`.\n",
        "  - `max_k`: Upper bound for the number of shots for the generated tasks at meta-train time. Only used if `k` is `null`. It must be greater or equal to `min_k`.\n",
        "  - `query_images_per_class`: Number of examples per class to include in the query set at meta-train time. It cannot be greater than 20.\n",
        "- `validation_datasets`: Number of datasets to be used for the meta-valid split. The split between train and validation datasets is done randomly, *i.e.*, if there are 10 datasets available for training, and you specify `validation_datasets: 3`, 3 out of the 10 available datasets will be randomly selected to create the meta-valid split. If you do not want to use meta-validation, you can either specify this configuration as `null` or 0, but in that case, the `meta_valid_generator` that you will receive will be `None`. It cannot be greater than 9. \n",
        "- `valid_config`: Configuration for the validation data.\n",
        "  - `N`: Fixed number of ways for the generated tasks at meta-validation time. If you would like to use any-way configuration, then you have to define this parameter as `null`. It cannot be less than 2.\n",
        "  - `min_N`: Lower bound for the number of ways for the generated tasks at meta-validation time. Only used if `N` is `null`. It cannot be less than 2 and must be less than or equal to `max_N`.\n",
        "  - `max_N`: Upper bound for the number of ways for the generated tasks at meta-validation time. Only used if `N` is `null`. It must be greater or equal to `min_N`.\n",
        "  - `k`: Fixed number of shots for the generated tasks at meta-validation time. If you would like to use any-shot configuration, then you have to define this parameter as `null`. It cannot be less than 1.\n",
        "  - `min_k`: Lower bound for the number of shots for the generated tasks at meta-validation time. Only used if `k` is `null`. It cannot be less than 1 and must be less than or equal to `max_k`.\n",
        "  - `max_k`: Upper bound for the number of shots for the generated tasks at meta-validation time. Only used if `k` is `null`. It must be greater or equal to `min_k`.\n",
        "  - `query_images_per_class`: Number of examples per class to include in the query set at meta-validation time. It cannot be greater than 20.\n",
        "\n",
        "**Note:** In the case of **tasks** (meta-train or meta-validation), if the configurations that you specify are greater than the maximum available number of classes in case of `N` or the maximum available number of examples per class in case of `k`, these values will be automatically adjusted to the available data. \n",
        "\n",
        "<font color=\"red\">**IMPORTANT:**</font> The `config.json` file is OPTIONAL, if you do not include it, the default configuration will be used:\n",
        "\n",
        "**Default configuration**\n",
        "```bash\n",
        "{\n",
        "    \"train_data_format\": \"task\",\n",
        "    \"batch_size\": null,\n",
        "    \"train_config\": {\n",
        "        \"N\": 5,\n",
        "        \"min_N\": null,\n",
        "        \"max_N\": null,\n",
        "        \"k\": null,\n",
        "        \"min_k\": 1,\n",
        "        \"max_k\": 20,\n",
        "        \"query_images_per_class\": 20\n",
        "    },\n",
        "    \"validation_datasets\": 0,\n",
        "    \"valid_config\": {\n",
        "        \"N\": null,\n",
        "        \"min_N\": 2,\n",
        "        \"max_N\": 20,\n",
        "        \"k\": null,\n",
        "        \"min_k\": 1,\n",
        "        \"max_k\": 20,\n",
        "        \"query_images_per_class\": 20\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "To better understand how the configuration affects the `meta_train_generator` and `meta_valid_generator`, the following code allow you to test and visualize your preferred configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QkrDk_U5Z5V"
      },
      "outputs": [],
      "source": [
        "from tutorial_utils import initialize_generators, plot_data\n",
        "\n",
        "path_to_public_data = \"public_data\" # Do not update this value unless you have \n",
        "                                    # moved the downloaded Public Data \n",
        "config = {\n",
        "    \"train_data_format\": \"task\",\n",
        "    \"batch_size\": None,\n",
        "    \"train_config\": {\n",
        "        \"N\": 10,\n",
        "        \"min_N\": None,\n",
        "        \"max_N\": None,\n",
        "        \"k\": None,\n",
        "        \"min_k\": 5,\n",
        "        \"max_k\": 10,\n",
        "        \"query_images_per_class\": 5\n",
        "    },\n",
        "    \"validation_datasets\": 2, # Since the public data has only 5 datasets for \n",
        "                              # meta-training, this parameter cannot be greater \n",
        "                              # than 4. However, in your submissions to the \n",
        "                              # Competition Site you can use up to 9 validation \n",
        "                              # datasets\n",
        "    \"valid_config\": {\n",
        "        \"N\": None,\n",
        "        \"min_N\": 2,\n",
        "        \"max_N\": 5,\n",
        "        \"k\": 20,\n",
        "        \"min_k\": None,\n",
        "        \"max_k\": None,\n",
        "        \"query_images_per_class\": 10\n",
        "    }\n",
        "}\n",
        "\n",
        "# During the competition, you do not have to initialize the generators, rather \n",
        "# you will automatically receive the initialized generators in the meta_fit \n",
        "# method of the MetaLeaner class. The only thing that you have to do if you want\n",
        "# to use your custom data configuration is to include the config.json file as \n",
        "# previously explained \n",
        "meta_train_generator, meta_valid_generator = initialize_generators(config, \n",
        "    path_to_public_data)\n",
        "\n",
        "\n",
        "# Visualization of the generated data\n",
        "print(\"Examples of the data generated with meta_train_generator\")\n",
        "number_of_examples_to_visualize = 1\n",
        "for i, data in enumerate(meta_train_generator(number_of_examples_to_visualize)):\n",
        "    plot_data(data, i)\n",
        "\n",
        "if meta_valid_generator is not None:\n",
        "    print(\"\\n\\nExamples of the data generated with meta_valid_generator\")\n",
        "    for i, data in enumerate(meta_valid_generator(number_of_examples_to_visualize)):\n",
        "        plot_data(data, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jNNTnbv-Sdr"
      },
      "source": [
        "<a name='intermediate_finetuning'></a>\n",
        "## 2.5 Fine-tuning Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fZ3v3Br-flN"
      },
      "source": [
        "In the [beginner tutorial](#beginner_submission) you examined the **Random** baseline which as its name indicates just outputs random predictions. In this tutorial you will explore the **Fine-tuning** baseline which pre-trains a network with batches of data from the meta-training split and during meta-testing only fine-tunes the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjYb7n4D5Z73"
      },
      "outputs": [],
      "source": [
        "from tutorial_utils import display\n",
        "\n",
        "display(\"baselines/finetuning/model.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4xwHX75P1I9"
      },
      "source": [
        "The above example shows how the **MetaLearner** uses the `meta_train_generator` to produce a **Learner** that can be further adapted to unseen tasks. Additionally, the `meta_valid_generator` is used to select the best **Learner** and avoid overfitting since the best **Learner** is not necessarily the one at the end of all meta-train steps but rather the one with the highest performance on the meta-validation set.\n",
        "\n",
        "Moreover, you can check how the **Learner** can be saved and loaded. These two operations are crucial since the `ingestion program` will re-instantiate your **Learner** in each meta-test task, and the `load` method will be responsible for initializing it.\n",
        "\n",
        "Lastly, you can see how the **Learner** is trained on the support set of a meta-test task to produce a **Predictor** that returns the predictions (in the form of a probability matrix rather than raw predictions) based on the query set of the same task.\n",
        "\n",
        "**Note:** Remember that in this competition, it is MANDATORY that you seeded your algorithms. Make sure there are NO OTHER HARD CODED RANDOM SEEDS in your code.\n",
        "\n",
        "As in the [beginner tutorial](#beginner_submission) you can use the following cell to zip the Fine-tuning baseline and submit it on the [CodaLab platform](https://codalab.lisn.upsaclay.fr/competitions/3627#participate) from July 1st. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1NoPjIF5Z-P"
      },
      "outputs": [],
      "source": [
        "from tutorial_utils import zipdir\n",
        "\n",
        "model_dir = \"baselines/finetuning/\"\n",
        "submission_filename = \"finetuning_baseline.zip\"\n",
        "zipdir(submission_filename, model_dir)\n",
        "print(f\"Your file {submission_filename} is ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmNDO1TWFh93"
      },
      "source": [
        "## 2.6 Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JKnMGO4dy1u"
      },
      "source": [
        "Congratulations, you finished the intermediate tutorial. At this point, you should have clear understanding of the challenge API that your algorithms must follow and the way of specifying your preferred configurations for the meta-training and meta-validation data. Furthermore, you should also have the the Fine-tuning baseline ready to be submitted to the [Competition Site](https://codalab.lisn.upsaclay.fr/competitions/3627#participate-submit_results) from July 1st. Now, you can continue with the [Advanced tutorial](#advanced).\n",
        "\n",
        "If you run into bugs or issues when using this starting kit, please create issues on the [*Issues* page](https://github.com/DustinCarrion/cd-metadl/issues) of this competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yWjjA5sFh93"
      },
      "source": [
        "<hr style=\"border-width:4px;border-color:#84C7F7\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4tOn1qfFh94"
      },
      "source": [
        "<a name='advanced'></a>\n",
        "# 3 Advanced Level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33QFnAlxoh8Z"
      },
      "source": [
        "We assume you read the [intermediate tutorial](#intermediate). We now go over more advanced topics: \n",
        "\n",
        "1. [Formal Definitions](#advanced_definitions)\n",
        "2. [Logger](#advanced_logger)\n",
        "3. [Prototypical Networks Baseline](#advanced_proto)\n",
        "4. [Local Testing](#advanced_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSlQfK1BeJmz"
      },
      "source": [
        "<a name='advanced_definitions'></a>\n",
        "## 3.1 Formal Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3siB2qyeLVo"
      },
      "source": [
        "In the [beginner](#beginner_intro) and [intermediate](#intermediate_config) tutorials, we have been referring to **tasks** and **batches**, but we never formally defined them. Therefore, for completeness, this section presents the formal definition for both terms. \n",
        "\n",
        "Tasks are individual mini classification problems defined in the few-shot learning setting we are considering in this challenge. The meta-testing data will always be split into tasks. However, you have a choice to either split the meta-training data also into tasks or split it into batches (similarly to what is usually done in \"regular\" learning problems).\n",
        "\n",
        "**Task:** It represents a ***N*-way *k*-shot task** and it is defined as $ \\mathcal{T_j} = \\{ \\mathcal{D}_{\\mathcal{T_j}}^{train}, \\mathcal{D}_{\\mathcal{T_j}}^{test}\\}$, where $\\mathcal{D}_{\\mathcal{T_j}}^{train}$ corresponds to the *support set* that contains *N* classes (ways) and *k* examples per class (shots) for the *j*th task $\\mathcal{T_j}$, and $\\mathcal{D}_{\\mathcal{T_j}}^{test}$ is the *query set* that contains the labeled test examples of the same *N* classes for $\\mathcal{T_j}$. Since this competition is focused on the cross-domain few-shot learning setting, the data contained in one task belongs strictly to one dataset, but different tasks may come from different datasets because the meta-training split is composed of multiple datasets, *i.e.*, $\\mathcal{M}_{\\mathcal{D}}^{train} = \\{\\mathcal{D}_1, \\dots, \\mathcal{D}_n\\}$. The number of datasets ($n$) in the meta-training split $\\mathcal{M}_{\\mathcal{D}}^{train}$ depends on the number of datasets you want to use for the meta-validation split. During the **public phase** you will have **5 datasets** that you can use for meta-training and meta-validation (*e.g.*  $\\mathcal{M}_{\\mathcal{D}}^{train} = \\{\\mathcal{D}_1, \\mathcal{D}_2, \\mathcal{D}_3\\}$ and $\\mathcal{M}_{\\mathcal{D}}^{valid} = \\{\\mathcal{D}_4, \\mathcal{D}_5\\}$) because the remaining 5 datasets are used for the meta-testing split ($\\mathcal{M}_{\\mathcal{D}}^{test} = \\{\\mathcal{D}_6, \\dots, \\mathcal{D}_{10}\\}$). However, during the **feedback phase**, you will have **10 datasets** that you can use for meta-training and meta-validation, and during the **final phase**, you will have **20 datasets** for the same purpose. \n",
        "\n",
        "**Batch:** It is a collection of sampled examples from the meta-training data. The meta-training data is first concatenated to create a single large dataset including all classes, from which batches of data are sampled, *i.e.*, $\\mathcal{D}^{train} = concat(\\mathcal{D}_1, \\dots, \\mathcal{D}_n)$. As before, the public data is divided into **5 datasets** for the **meta-train split** and **5 datasets** for the **meta-test split** (the meta-train split can be further divided into meta-train and meta-validation).\n",
        "\n",
        "Generally, the methods used to tackle the few-shot learning problem differ in the way the preprocessing layers of the backbone are trained and the type of classifier used. The preprocessing layers (computing a feature embedding) are meta-trained either in an \"episodic\" manner (making use of a split into tasks) or in a \"regular\" batch training manner. Therefore, since you can select the data format during meta-training, you can perform these two meta-training approaches. For example, the [Fine-tuning baseline](#intermediate_finetuning) introduced in the intermediate tutorial uses the regular batch training, but the [Prototypical Networks baseline](#advanced_proto) covered in the next section uses \"episodic\" training.\n",
        "\n",
        "**Note:** The division of the public data can be found in the `public_data/info/meta_splits.txt` file. We strongly recommend that you do not alter this file unless you are completely sure you understand your changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZyNu7nrL_S7"
      },
      "source": [
        "<a name='advanced_logger'></a>\n",
        "## 3.2 Logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI0q30kCMQBz"
      },
      "source": [
        "In the [intermediate tutorial](#intermediate_API), we explained the Challenge API, and we introduced the **MetaLearner** $\\rightarrow$ **Learner** $\\rightarrow$ **Predictor** structure. In this tutorial, we will focus on a very important operation that can be performed by the **MetaLearner**, logging the performance of the meta-learning process. \n",
        "\n",
        "At its initialization, the **MetaLearner** always receives three parameters:\n",
        "\n",
        "* `train_classes` (int): Total number of classes that can be seen during meta-training. If the data format during training is **task**, then this parameter corresponds to the number of ways, while if the data format is **batch**, this parameter corresponds to the total number of classes across all training datasets.\n",
        "* `total_classes` (int): Total number of classes across all training datasets. If the data format during training is **batch** this parameter is exactly the same as `train_classes`.\n",
        "* `logger` (Logger): Logger that you can use during meta-learning (**HIGHLY RECOMMENDED**). You can use it after each meta-train or meta-validation iteration as explained below.\n",
        "\n",
        "Thanks to the parent initialization of the **MetaLearner**:\n",
        "```python\n",
        "super().__init__(train_classes, total_classes, logger)\n",
        "```\n",
        "\n",
        "You will have access to the method `self.log(data, predictions, loss, meta_train)` where:   \n",
        "* `data` (task or batch): It is the data used in the current iteration.\n",
        "* `predictions` (np.ndarray): Predictions associated to each test example in the specified data. It can be the raw logits matrix (the logits are the unnormalized final scores of your model), a probability matrix, or the predicted labels.\n",
        "* `loss` (float, optional): Loss of the current iteration. Defaults to None.\n",
        "- `meta_train` (bool, optional): Boolean flag to control if the current iteration belongs to meta-training. Defaults to True.\n",
        "\n",
        "This method will:\n",
        "1. Print detailed performance logs that you can see in the `View ingestion output log` option of the [Competition Site](https://codalab.lisn.upsaclay.fr/competitions/3627#participate-submit_results) after your submission finishes its execution. The logs will contain: \n",
        "    - The number of the meta-train or meta-valid iteration. \n",
        "    - The performance of the iteration, which in the case of **tasks**, will be the **Normalized Accuracy** and **Accuracy** ([see below](#advanced_metrics) for the definitions), and for **batches**, it will only include the Accuracy.\n",
        "    - The loss if it is specified when calling the `self.log(data, predictions, loss, meta_train)` method.\n",
        "    - If the data is a task, the logs will also contain the task configuration, i.e., \\[*N*-way *k*-shot task from *dataset*\\]. \n",
        "\n",
        "    An example of logs is shown below:\n",
        "\n",
        "```bash\n",
        "Meta-train iteration 1:\t0.2375 (Normalized Accuracy)\t0.3900 (Accuracy)\t0.0105 (Loss)\t[5-way 10-shot task from FLW]\n",
        "Meta-train iteration 2:\t0.3250 (Normalized Accuracy)\t0.4600 (Accuracy)\t0.0230 (Loss)\t[5-way 10-shot task from BCT]\n",
        "Meta-train iteration 3:\t0.3500 (Normalized Accuracy)\t0.4800 (Accuracy)\t0.0220 (Loss)\t[5-way 10-shot task from BCT]\n",
        "Meta-train iteration 4:\t0.3875 (Normalized Accuracy)\t0.5100 (Accuracy)\t0.0145 (Loss)\t[5-way 10-shot task from FLW]\n",
        "Meta-train iteration 5:\t0.3250 (Normalized Accuracy)\t0.4600 (Accuracy)\t0.0282 (Loss)\t[5-way 10-shot task from BCT]\n",
        "\n",
        "############################## Meta-valid step 1 ##############################\n",
        "Meta-valid iteration 1:\t0.4000 (Normalized Accuracy)\t0.5200 (Accuracy)\t[5-way 5-shot task from BRD]\n",
        "Meta-valid iteration 2:\t0.1125 (Normalized Accuracy)\t0.2900 (Accuracy)\t[5-way 5-shot task from CRS]\n",
        "Meta-valid iteration 3:\t0.2875 (Normalized Accuracy)\t0.4300 (Accuracy)\t[5-way 5-shot task from BRD]\n",
        "Meta-valid iteration 4:\t0.2375 (Normalized Accuracy)\t0.3900 (Accuracy)\t[5-way 5-shot task from BRD]\n",
        "###############################################################################\n",
        "\n",
        "Meta-train iteration 6:\t0.5000 (Normalized Accuracy)\t0.6000 (Accuracy)\t0.0071 (Loss)\t[5-way 10-shot task from BCT]\n",
        "Meta-train iteration 7:\t0.4250 (Normalized Accuracy)\t0.5400 (Accuracy)\t0.0117 (Loss)\t[5-way 10-shot task from FLW]\n",
        "Meta-train iteration 8:\t0.4625 (Normalized Accuracy)\t0.5700 (Accuracy)\t0.0045 (Loss)\t[5-way 10-shot task from BCT]\n",
        "Meta-train iteration 9:\t0.4375 (Normalized Accuracy)\t0.5500 (Accuracy)\t0.0058 (Loss)\t[5-way 10-shot task from FLW]\n",
        "Meta-train iteration 10:\t0.4500 (Normalized Accuracy)\t0.5600 (Accuracy)\t0.0032 (Loss)\t[5-way 10-shot task from BCT]\n",
        "```\n",
        "\n",
        "2. Save detailed logs in a `logs` folder located inside the ingestion output. This output can be downloaded using the `Download output from prediction step` option of the [Competition Site](https://codalab.lisn.upsaclay.fr/competitions/3627#participate-submit_results) after your submission finishes its execution. The `logs` folder is organized as follows:\n",
        "```bash\n",
        "logs/\n",
        "│   meta_train/\n",
        "|   |   ground_truth/           <- Real labels for each meta-train iteration in NumPy format.\n",
        "|   |   |   iteration_1.out\n",
        "|   |   |   iteration_2.out\n",
        "|   |   |   ...\n",
        "|   |   predictions/            <- Your predictions for each meta-train iteration in NumPy format.\n",
        "|   |   |   iteration_1.out\n",
        "|   |   |   iteration_2.out\n",
        "|   |   |   ...\n",
        "|   |   performance.csv         <- Performance for each meta-train iteration.\n",
        "|   |   tasks.csv               <- Task configuration for each meta-train iteration (Only saved if tasks are used during meta-training).\n",
        "│   meta_validation/\n",
        "|   |   step_1/                 <- Results for the first meta-validation step.\n",
        "|   |   |   performance.csv     <- Performance for each meta-valid iteration.\n",
        "|   |   |   task.csv            <- Task configuration for each meta-valid iteration.\n",
        "|   |   step_2/                 <- Results for the second meta-validation step.\n",
        "|   |   |   performance.csv     <- Performance for each meta-valid iteration.\n",
        "|   |   |   task.csv            <- Task configuration for each meta-valid iteration.\n",
        "|   |   ...\n",
        "│   experimental_settings.txt   <- Detailed information about the data configuration in each phase (meta-train, meta-valid, meta-test).\n",
        "```\n",
        "\n",
        "You can read the NumPy files (`.out`) with `np.loadtxt(file, dtype=dtype)`, where `file` is the path to the file you want to read, and `dtype` is the format of the data (ground truth values are always `int`, and predictions can be either `int` or `float`). \n",
        "\n",
        "<a name='advanced_metrics'></a>\n",
        "The **Normalized Accuracy** is defined as:\n",
        "\n",
        "$$\n",
        "\\textrm{Normalized Accuracy} = \\frac{bac - bac_{RG}}{1-bac_{RG}},\n",
        "$$\n",
        "\n",
        "where $bac$ is the average accuracy per class (macro-averaging recall) defined as:\n",
        "\n",
        "$$\n",
        "bac = \\frac{1}{num\\_ways} \\sum_{i=1}^{num\\_ways}{\\frac{\\textrm{correctly classified examples of class } i}{\\textrm{total examples of class } i}},\n",
        "$$\n",
        "\n",
        "and $bac_{RG}$ is accuracy of random guessing $\\frac{1}{num\\_ways}$.\n",
        "\n",
        "On the other hand the **Accuracy** is defined as:\n",
        "\n",
        "$$\n",
        "\\textrm{Accuracy} = \\frac{\\textrm{correctly classified examples}}{\\textrm{total examples}}.\n",
        "$$\n",
        "\n",
        "You can find a real usage example of the Logger in the [next section](#advanced_proto).\n",
        "\n",
        "**Note:** When following the [Local Testing](#advanced_test) section, you will find the `logs` folder inside the directory that you specify as `output_dir_ingestion`.\n",
        "\n",
        "<font color=\"red\">IMPORTANT:</font> Using the Logger is not mandatory, but we encourage you to use it to easily analyze the performance evolution of your methods during meta-learning and help you identify possible problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABlZHx5GsIKw"
      },
      "source": [
        "<a name='advanced_proto'></a>\n",
        "## 3.3 Prototypical Networks Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Wy44bCsbK3"
      },
      "source": [
        "In the [intermediate tutorial](#intermediate_finetuning), you examined the **Fine-tuning** baseline, which pre-trains a network with batches of data from the meta-training split, and during meta-testing, only fine-tunes the last layer. In this tutorial, you will explore the **Prototypical Newotks** baseline based on  [J. Snell et al. - Prototypical Networks for Few-shot Learning (2017)](https://arxiv.org/pdf/1703.05175).\n",
        "\n",
        "\n",
        "The following figure, taken from the [Prototypical Networks for Few-shot Learning paper](https://arxiv.org/pdf/1703.05175), summarizes the procedure of assigning a class to a new unlabelled observation.\n",
        "\n",
        "<center>\n",
        "<img src=\"./imgs/protonet.png\" alt=\"Prototypical Networks\" width=400>\n",
        "</center>\n",
        "\n",
        "In a nutshell, what Prototypical Networks does is:\n",
        "\n",
        "1. Project the images of the support set into the feature space (output embeddings of the selected neural network). In the above figure, the projected images correspond to each colored dot (green, orange, and purple); different colors represent different classes. Thus, the figure is showing a 3-ways 5-shots problem since there are 3 classes (colors) with 5 examples per class (dots per color). Additionally, the projection would be in 2 dimensions (2 features).\n",
        "2. Compute the prototypes for each class. The prototypes are the mean vector of all examples of the same class. In the figure the prototypes are the black dots denoted as $c_1, c_2,$ and $c_3$.\n",
        "3. Project the new unlabelled observation $x$ into the feature space (the white dot in the figure).\n",
        "4. Assign the projected $x$ to the class whose centroid is closest ($c_2$ in the figure).\n",
        "\n",
        "During meta-training, the backbone is trained in an \"episodic\" by minimizing the distance between the projections of the query set images and their corresponding prototypes. Please refer to the [original paper](https://arxiv.org/pdf/1703.05175) to see the full details of this method. Below you can see our implementation of Prototypical Networks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPrlV4Vc7qvu"
      },
      "outputs": [],
      "source": [
        "from tutorial_utils import display\n",
        "\n",
        "display(\"baselines/protonet/model.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz6F6yxL8HqN"
      },
      "source": [
        "As in the previous tutorials you can use the following cell to zip this baseline and submit it on the [CodaLab platform](https://codalab.lisn.upsaclay.fr/competitions/3627#participate) from July 1st. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m939Ubwr8UkP"
      },
      "outputs": [],
      "source": [
        "from tutorial_utils import zipdir\n",
        "\n",
        "model_dir = \"baselines/protonet/\"\n",
        "submission_filename = \"protonet_baseline.zip\"\n",
        "zipdir(submission_filename, model_dir)\n",
        "print(f\"Your file {submission_filename} is ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlpdS9aX8jDa"
      },
      "source": [
        "Until this point, you have checked 3 baselines. However, we provide you 6 baselines that you can locate in the `baselines/` folder. The baselines are:\n",
        "\n",
        "- The **Random** baseline, which, as its name indicates, just outputs random predictions.\n",
        "- The **Train from scratch** baseline, which learns every task starting from a specified initialization at meta-test time, *i.e.*, no meta-learning.  \n",
        "- The **Fine-tuning** baseline, which pre-trains a network with batches of data from the meta-training split, and during meta-testing, only fine-tunes the last layer.\n",
        "- The **Prototypical Networks** based on  [J. Snell et al. - Prototypical Networks for Few-shot Learning (2017)](https://arxiv.org/pdf/1703.05175).\n",
        "- The **Matching Networks** based on  [O. Vinyals et al. - Matching Networks for One Shot Learning (2017)](https://arxiv.org/pdf/1606.04080).\n",
        "- The **MAML** algorithm based on [C. Finn et al. - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (2017)](https://arxiv.org/pdf/1703.03400).\n",
        "\n",
        "You can use the following cell to visualize and zip any baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1M3k8qu9kmu"
      },
      "outputs": [],
      "source": [
        "from tutorial_utils import display, zipdir\n",
        "\n",
        "baseline = \"Train from scratch\"  # The options are: Random, Train from scratch, \n",
        "                                 # Fine-tuning, Prototypical Networks, \n",
        "                                 # Matching Networks, and MAML\n",
        "zip_baseline = False  # Change to True if you want to zip the baseline\n",
        "\n",
        "baseline_names = {\n",
        "    \"Random\": \"random\",\n",
        "    \"Train from scratch\": \"train_from_scratch\",\n",
        "    \"Fine-tuning\": \"finetuning\",\n",
        "    \"Prototypical Networks\": \"protonet\",\n",
        "    \"Matching Networks\": \"matchingnet\",\n",
        "    \"MAML\": \"maml\"\n",
        "}\n",
        "\n",
        "model_dir = f\"baselines/{baseline_names[baseline]}/\"\n",
        "display(f\"{model_dir}/model.py\")\n",
        "\n",
        "if zip_baseline:\n",
        "    submission_filename = f\"{baseline_names[baseline]}_baseline.zip\"\n",
        "    zipdir(submission_filename, model_dir)\n",
        "    print(f\"\\n\\n\\nYour file {submission_filename} is ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIepIACZeZJH"
      },
      "source": [
        "<a name='advanced_test'></a>\n",
        "## 3.4 Local Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOFatcFYebFl"
      },
      "source": [
        "In this tutorial, we will use the [Public Data](#intermediate_public_data) to test your algorithms directly in your computer before submitting them to the [Competition Site](https://codalab.lisn.upsaclay.fr/competitions/3627#participate). Please make sure you have followed the [Public Data](#intermediate_public_data) and the [Configuring the environment](#intermediate_env) sections.\n",
        "\n",
        "To locally test your algorithms, we provide you the `run.py` script that you can locate inside the `cdmetadl` directory. This script is meant to mimick what is happenning on the CodaLab Platform (see the [Evaluation workflow of the competition](#beginner_workflow)). More specifically, it will create your **MetaLearner** object, run the `meta_fit()` to produce the **Learner**; then, for each task in the meta-testing stage, it will be trained with the support set to create the **Predictor**, which will be evaluated with the unlabelled query set of the same task.\n",
        "\n",
        "The `run.py` script receives several arguments of which the most important are:\n",
        "\n",
        "- `input_data_dir`: The path to the directory that contains the **Public data**. \n",
        "- `submission_dir`: The path to the directory that contains your **algorithm's code** following the format we previously defined. \n",
        "- `output_dir_ingestion`: The path to the directory that will store the results of the ingestion program.\n",
        "- `output_dir_scoring`: The path to the directory that will store the results of the scoring program.\n",
        "- `verbose`: Boolean flag to control the verbosity of the script.\n",
        "- `overwrite_previous_results`: Boolean flag to control whether the output directories should be overwritten or not. If True, then every time you run the `run.py` script, it will overwrite the previous results. If False, then every time you run the `run.py` script, it will move the previous results to other directories to avoid overwriting them.\n",
        "- `test_tasks_per_dataset`: Number of tasks per dataset during the meta-testing stage. In the competition, during the Feedback phase this parameter is set to 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8F2L6tzgFSW"
      },
      "outputs": [],
      "source": [
        "!python -m cdmetadl.run \\\n",
        "    --input_data_dir=public_data \\\n",
        "    --submission_dir=baselines/protonet \\\n",
        "    --output_dir_ingestion=ingestion_output \\\n",
        "    --output_dir_scoring=scoring_output \\\n",
        "    --verbose=False \\\n",
        "    --overwrite_previous_results=True \\\n",
        "    --test_tasks_per_dataset=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7D_aNnWec3k"
      },
      "source": [
        "You can check your detailed results running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0drpZT1KeeiA"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(filename=\"scoring_output/detailed_results.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKkRzi_FefeT"
      },
      "source": [
        "## 3.5 Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGlVjdUHegnc"
      },
      "source": [
        "Congratulations, you finished all the tutorials. At this point you should have a clear understanding of the challenge and you should be ready to start working on your own algorithms. We recommend the following steps:\n",
        "\n",
        "1. Check the details of all baselines provided (`baselines/`) to familiarize yourself with the competition API.\n",
        "2. Play with the baselines parameters.\n",
        "3. Read the [lessons learned from the previous challenge (MetaDL @ NeurIPS 2021)](https://hal.archives-ouvertes.fr/hal-03688638), and [download the solutions of the winners](https://metalearning.chalearn.org/metadlneurips2021).\n",
        "4. Read about the datasets we use in this competition and the baseline results in our [Meta-Album preprint](https://meta-album.github.io/paper/Meta-Album.pdf).\n",
        "\n",
        "If you run into bugs or issues when using this starting kit, please create issues on the [*Issues* page](https://github.com/DustinCarrion/cd-metadl/issues) of this competition."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cSlQfK1BeJmz"
      ],
      "name": "Cross-Domain MetaDL Tutorial",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('test_tutorial')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "497421338d5a991e9ce0430d2a85b4d7d568613d724beee1ec5a0f5b9e879bcc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
